"""Result Cache Manager - Caches execution results for performance optimization"""

import json
import hashlib
from typing import Optional, Any, Dict
from uuid import UUID
from datetime import datetime, timedelta, timezone
from decimal import Decimal
from redis.asyncio import Redis
from pydantic import BaseModel, Field, ConfigDict


class CachedResult(BaseModel):
    """Cached execution result with metadata"""
    model_config = ConfigDict(
        json_encoders={
            datetime: lambda v: v.isoformat(),
            UUID: lambda v: str(v),
            Decimal: lambda v: float(v)
        }
    )
    
    result: Dict[str, Any]
    cached_at: datetime
    cache_key: str
    tool_id: UUID
    tool_name: str
    ttl_seconds: int
    hit_count: int = 0


class CacheStats(BaseModel):
    """Cache statistics"""
    total_entries: int
    total_hits: int
    total_misses: int
    hit_rate: float
    total_size_bytes: int
    eviction_count: int


class ResultCacheManager:
    """
    Result Cache Manager provides caching for MCP tool execution results.
    
    Responsibilities:
    - Generate cache keys based on tool ID and parameters
    - Store execution results with TTL
    - Retrieve cached results with metadata
    - Invalidate cache on tool configuration changes
    - Implement LRU eviction policy
    
    Requirements: 10.1, 10.2, 10.3, 10.4, 10.5
    """
    
    # Default cache TTL (1 hour)
    DEFAULT_TTL = 3600
    
    # Maximum cache size (number of entries)
    MAX_CACHE_SIZE = 10000
    
    # Cache key prefixes
    RESULT_PREFIX = "cache:result:"
    METADATA_PREFIX = "cache:metadata:"
    STATS_KEY = "cache:stats"
    LRU_KEY = "cache:lru"
    
    def __init__(self, redis: Redis):
        """
        Initialize Result Cache Manager.
        
        Args:
            redis: Redis client instance
        """
        self.redis = redis
    
    # ========================================================================
    # Cache Key Generation (Requirement 10.1)
    # ========================================================================
    
    @staticmethod
    def generate_cache_key(
        tool_id: UUID,
        tool_name: str,
        arguments: Dict[str, Any]
    ) -> str:
        """
        Generate deterministic cache key from tool ID and parameters.
        
        The cache key is generated by:
        1. Sorting arguments to ensure consistent ordering
        2. Creating a JSON string representation
        3. Hashing with SHA256 for fixed-length key
        
        Args:
            tool_id: Tool identifier
            tool_name: Tool name
            arguments: Tool execution arguments
            
        Returns:
            SHA256 hash of the cache key components
            
        Validates: Requirements 10.1
        """
        # Sort arguments for consistent key generation
        sorted_args = json.dumps(arguments, sort_keys=True, default=str)
        
        # Create key data string
        key_data = f"{tool_id}:{tool_name}:{sorted_args}"
        
        # Generate SHA256 hash
        return hashlib.sha256(key_data.encode()).hexdigest()
    
    # ========================================================================
    # Cache Storage (Requirement 10.1)
    # ========================================================================
    
    async def store_result(
        self,
        cache_key: str,
        result: Dict[str, Any],
        tool_id: UUID,
        tool_name: str,
        ttl: Optional[int] = None
    ) -> None:
        """
        Store execution result in cache with TTL.
        
        Implements LRU eviction by:
        1. Checking cache size before storing
        2. Evicting least-recently-used entry if at capacity
        3. Storing result with TTL
        4. Updating LRU tracking
        
        Args:
            cache_key: Cache key for the result
            result: Execution result to cache
            tool_id: Tool identifier
            tool_name: Tool name
            ttl: Time to live in seconds (default: DEFAULT_TTL)
            
        Validates: Requirements 10.1, 10.5
        """
        ttl = ttl or self.DEFAULT_TTL
        
        # Check cache size and evict if necessary (LRU)
        await self._enforce_cache_limit()
        
        # Create cached result object
        cached_result = CachedResult(
            result=result,
            cached_at=datetime.now(timezone.utc),
            cache_key=cache_key,
            tool_id=tool_id,
            tool_name=tool_name,
            ttl_seconds=ttl,
            hit_count=0
        )
        
        # Serialize result
        result_json = cached_result.model_dump_json()
        
        # Store result with TTL
        result_key = f"{self.RESULT_PREFIX}{cache_key}"
        await self.redis.setex(result_key, ttl, result_json)
        
        # Update LRU tracking (sorted set with timestamp as score)
        current_time = datetime.now(timezone.utc).timestamp()
        await self.redis.zadd(self.LRU_KEY, {cache_key: current_time})
        
        # Update statistics
        await self._increment_stat("total_entries", 1)
    
    # ========================================================================
    # Cache Retrieval (Requirement 10.2, 10.3)
    # ========================================================================
    
    async def get_cached_result(
        self,
        cache_key: str
    ) -> Optional[CachedResult]:
        """
        Retrieve cached result if available and within TTL.
        
        Returns cached result with metadata including:
        - Cache hit indicator
        - Result age
        - Hit count
        
        Args:
            cache_key: Cache key to retrieve
            
        Returns:
            CachedResult if found and valid, None otherwise
            
        Validates: Requirements 10.2, 10.3
        """
        result_key = f"{self.RESULT_PREFIX}{cache_key}"
        
        # Retrieve from Redis
        cached_data = await self.redis.get(result_key)
        
        if not cached_data:
            # Cache miss
            await self._increment_stat("total_misses", 1)
            return None
        
        # Parse cached result
        cached_result = CachedResult.model_validate_json(cached_data)
        
        # Increment hit count
        cached_result.hit_count += 1
        
        # Update LRU tracking (mark as recently used)
        current_time = datetime.now(timezone.utc).timestamp()
        await self.redis.zadd(self.LRU_KEY, {cache_key: current_time})
        
        # Update statistics
        await self._increment_stat("total_hits", 1)
        
        # Update hit count in cache
        updated_json = cached_result.model_dump_json()
        ttl = await self.redis.ttl(result_key)
        if ttl > 0:
            await self.redis.setex(result_key, ttl, updated_json)
        
        return cached_result
    
    # ========================================================================
    # Cache Invalidation (Requirement 10.4)
    # ========================================================================
    
    async def invalidate_tool_cache(
        self,
        tool_id: UUID
    ) -> int:
        """
        Invalidate all cached results for a specific tool.
        
        Called when tool configuration changes to ensure
        cached results don't reflect outdated behavior.
        
        Args:
            tool_id: Tool identifier
            
        Returns:
            Number of cache entries invalidated
            
        Validates: Requirements 10.4
        """
        invalidated_count = 0
        
        # Scan all cache keys
        pattern = f"{self.RESULT_PREFIX}*"
        cursor = 0
        
        while True:
            cursor, keys = await self.redis.scan(
                cursor,
                match=pattern,
                count=100
            )
            
            for key in keys:
                # Get cached result to check tool_id
                cached_data = await self.redis.get(key)
                if cached_data:
                    try:
                        cached_result = CachedResult.model_validate_json(cached_data)
                        if cached_result.tool_id == tool_id:
                            # Delete this cache entry
                            await self.redis.delete(key)
                            
                            # Remove from LRU tracking
                            cache_key = key.replace(self.RESULT_PREFIX, "")
                            await self.redis.zrem(self.LRU_KEY, cache_key)
                            
                            invalidated_count += 1
                    except Exception:
                        # Skip invalid entries
                        continue
            
            if cursor == 0:
                break
        
        # Update statistics
        await self._increment_stat("total_entries", -invalidated_count)
        
        return invalidated_count
    
    async def invalidate_all(self) -> int:
        """
        Invalidate all cached results.
        
        Returns:
            Number of cache entries invalidated
        """
        # Delete all result keys
        pattern = f"{self.RESULT_PREFIX}*"
        cursor = 0
        deleted_count = 0
        
        while True:
            cursor, keys = await self.redis.scan(
                cursor,
                match=pattern,
                count=100
            )
            
            if keys:
                await self.redis.delete(*keys)
                deleted_count += len(keys)
            
            if cursor == 0:
                break
        
        # Clear LRU tracking
        await self.redis.delete(self.LRU_KEY)
        
        # Reset statistics
        await self.redis.delete(self.STATS_KEY)
        
        return deleted_count
    
    # ========================================================================
    # LRU Eviction (Requirement 10.5)
    # ========================================================================
    
    async def _enforce_cache_limit(self) -> None:
        """
        Enforce cache size limit using LRU eviction.
        
        When cache reaches MAX_CACHE_SIZE, evicts the
        least-recently-used entry to make room for new entries.
        
        Validates: Requirements 10.5
        """
        # Get current cache size
        cache_size = await self.redis.zcard(self.LRU_KEY)
        
        if cache_size >= self.MAX_CACHE_SIZE:
            # Get least-recently-used entry (lowest score)
            lru_entries = await self.redis.zrange(
                self.LRU_KEY,
                0,
                0,
                withscores=False
            )
            
            if lru_entries:
                lru_cache_key = lru_entries[0]
                
                # Delete the result
                result_key = f"{self.RESULT_PREFIX}{lru_cache_key}"
                await self.redis.delete(result_key)
                
                # Remove from LRU tracking
                await self.redis.zrem(self.LRU_KEY, lru_cache_key)
                
                # Update statistics
                await self._increment_stat("eviction_count", 1)
                await self._increment_stat("total_entries", -1)
    
    # ========================================================================
    # Cache Statistics
    # ========================================================================
    
    async def get_cache_stats(self) -> CacheStats:
        """
        Get cache statistics.
        
        Returns:
            CacheStats with current cache metrics
        """
        # Get stats from Redis
        stats_data = await self.redis.hgetall(self.STATS_KEY)
        
        total_entries = int(stats_data.get("total_entries", 0))
        total_hits = int(stats_data.get("total_hits", 0))
        total_misses = int(stats_data.get("total_misses", 0))
        eviction_count = int(stats_data.get("eviction_count", 0))
        
        # Calculate hit rate
        total_requests = total_hits + total_misses
        hit_rate = (total_hits / total_requests * 100) if total_requests > 0 else 0.0
        
        # Estimate total size (approximate)
        total_size_bytes = 0
        pattern = f"{self.RESULT_PREFIX}*"
        cursor = 0
        
        while True:
            cursor, keys = await self.redis.scan(
                cursor,
                match=pattern,
                count=100
            )
            
            for key in keys:
                memory_usage = await self.redis.memory_usage(key)
                if memory_usage:
                    total_size_bytes += memory_usage
            
            if cursor == 0:
                break
        
        return CacheStats(
            total_entries=total_entries,
            total_hits=total_hits,
            total_misses=total_misses,
            hit_rate=round(hit_rate, 2),
            total_size_bytes=total_size_bytes,
            eviction_count=eviction_count
        )
    
    async def _increment_stat(self, stat_name: str, increment: int) -> None:
        """
        Increment a cache statistic.
        
        Args:
            stat_name: Name of the statistic
            increment: Amount to increment (can be negative)
        """
        await self.redis.hincrby(self.STATS_KEY, stat_name, increment)
    
    # ========================================================================
    # Utility Methods
    # ========================================================================
    
    async def get_cache_entry_count(self) -> int:
        """
        Get the current number of cached entries.
        
        Returns:
            Number of entries in cache
        """
        return await self.redis.zcard(self.LRU_KEY)
    
    async def get_ttl(self, cache_key: str) -> int:
        """
        Get remaining TTL for a cache entry.
        
        Args:
            cache_key: Cache key
            
        Returns:
            Remaining TTL in seconds, -1 if key doesn't exist, -2 if no TTL
        """
        result_key = f"{self.RESULT_PREFIX}{cache_key}"
        return await self.redis.ttl(result_key)
    
    async def exists(self, cache_key: str) -> bool:
        """
        Check if a cache entry exists.
        
        Args:
            cache_key: Cache key
            
        Returns:
            True if entry exists, False otherwise
        """
        result_key = f"{self.RESULT_PREFIX}{cache_key}"
        return bool(await self.redis.exists(result_key))
